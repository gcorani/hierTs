favorableProps$medianBaseBayesCorr[counter]  <- median(subresults$mseBase / subresults$mseBayesCorr)
favorableProps$medianMintBayesCorr[counter] <- median(subresults$mseCombMint / subresults$mseBayesCorr)
#generate the bplot with ggplot2
library(ggplot2)
pdfname <- paste("results/plot","_",dset,"_",fmethod,".pdf",sep = "")
denom <- subresults$mseBase
resLenght <- length(subresults$mseBase)
#old code, 3 models
# relMse <- rbind(matrix(subresults$mseCombMint/denom), matrix(subresults$mseBayes/denom), matrix(subresults$mseBayesCorr/denom))
# label <-  factor(rbind(matrix(rep("Mint",resLenght)),matrix(rep("Bayes",resLenght)),matrix(rep("Bayes (corr)",resLenght))),
# levels = c("Mint","Bayes","Bayes (corr)"))
#new code, 2 models (minT and Bayes corr)
relMse <- rbind(matrix(subresults$mseCombMint/denom), matrix(subresults$mseBayesCorr/denom))
label <-  factor(rbind(matrix(rep("Mint",resLenght)),matrix(rep("Bayes (corr)",resLenght))),
levels = c("Mint","Bayes (corr)"))
dataPlot <- as.data.frame(relMse)
dataPlot$label <- label
currentPlot <- ggplot(dataPlot, aes(x = label, y = log10(relMse))) + geom_boxplot()  +
stat_boxplot(geom = "errorbar", width = 0.5) +  #draw the whiskers
scale_x_discrete(name = "") +
scale_y_continuous(name = "Log10 ( mse / mse(base) ) ")
scaling <- 1.8 #to avoid large outliers that make the boxplot unreadable
if (dset=="tourism"){
scaling<- 1.1
}
else if (fmethod=="ets"){
scaling<- 3
}
ylim1 = boxplot.stats(log(dataPlot$V1))$stats[c(1, 5)]
currentPlot = currentPlot + coord_cartesian(ylim = ylim1*scaling)  + geom_hline(yintercept = 0, color='darkblue')
print(currentPlot)
ggsave(pdfname, width = 4, height = 3)
counter <- counter + 1
}
filename=paste("results/summary",dset,".csv",sep="")
write.table(favorableProps,file=filename,sep=",",row.names = FALSE)
return (favorableProps)
}
source('~/switchDrive/hierTs/hierarchicalTimeSeries/code/hierTs/parseHierResults_aggregatedH.R')
parseHierResults_aggregatedH("infantgts")
parseHierResults_aggregatedH("tourism")
print(paste("AEdemand: ",colnames(AEdemand)[i]))
freq <- frequency(AEdemand[,i])
n <- length(AEdemand[,i])
endTrain <- time(AEdemand[,i])[n - freq - currentExp ]
startTest <- time(AEdemand[,i])[n - freq - currentExp + 1]
train <- window(AEdemand[,i], end=endTrain)
test <- window(AEdemand[,i], start=startTest)
tsObj<- list()
tsObj$x <- train
tsObj$xx <- test
i
i<-1
currentExp<-
currentExp<-1
#prepares the AEdemand dataset and launches the temporal reconciliation experiment
#the fmethod is forced to be arima as the expsmoothing does not support such long seasonality
library(thief)
library(fpp2)
source("temporalRec.R")
#finestra di 40 esperimenti su 13-weeks ahead forecast
totExp=20
print(paste("AEdemand: ",colnames(AEdemand)[i]))
freq <- frequency(AEdemand[,i])
n <- length(AEdemand[,i])
endTrain <- time(AEdemand[,i])[n - freq - currentExp ]
startTest <- time(AEdemand[,i])[n - freq - currentExp + 1]
train <- window(AEdemand[,i], end=endTrain)
test <- window(AEdemand[,i], start=startTest)
tsObj<- list()
tsObj$x <- train
tsObj$xx <- test
tsObj$sn <- colnames(AEdemand)[i]
library(fpp2)
library(hts)
library(thief)
# computes mae for temporal hierarchies
#both actual and forecast are temporal hierarchies
#it  averages the  different time series having the same frequency
getHierMae <- function (actual, forecast) {
hierMae <- vector(length = length(forecast))
for (i in seq_along(forecast)) {
hierMae[i] <- mean( abs (forecast[[i]]$mean - actual[[i]]) )
}
return (hierMae)
}
globalMse <- function (actual, forecast) {
mse <- 0
for (i in seq_along(forecast)) {
mse <- mse + sum ( (forecast[[i]]$mean - actual[[i]])^2 )
}
return (mse)
}
calibration <- function (actual, forecast, sigmas) {
included50 <- as.vector(rep(0,length(actual)))
included80 <- as.vector(rep(0,length(actual)))
#implementation to be finalized
for (ii in seq_along(forecast)) {
upper50 <- as.numeric(fc[[i]]$upper[,"50%"])
lower50 <- as.numeric(fc[[i]]$lower[,"50%"])
included50[ii] <- (actual[ii] > lower) &  (actual[ii] < upper)
upper80 <- as.numeric(fc[[i]]$upper[,"80%"])
lower80 <- as.numeric(fc[[i]]$lower[,"80%"])
included80[ii] <- (actual[ii] > lower) &  (actual[ii] < upper)
}
return (mean(included50),mean(included80))
}
#builds the A matrix, which indicates  which bottom time series sum up to each upper time series.
buildMatrix <- function() {
A <- matrix(data = 0, nrow = length(upperIdx), ncol = length(bottomIdx))
maxFreq <- frequency(trainHier[[1]])
counter <- 1
for (ii in (2:length(trainHier))){
currentFreq <- frequency(trainHier[[ii]])
aggregatedTs <- currentFreq
howManyBottomToSum <- maxFreq / currentFreq
offset <- 1
for (jj in (1:aggregatedTs)) {
A[counter, offset : (offset + howManyBottomToSum - 1)] <- 1
offset <- offset + howManyBottomToSum
counter <- counter + 1
}
return (t(A))
}
#coverage of the PI is 0.8
alpha <- 0.2
#default test set for the M3 is 18 months for the monthly (tp be adapted) and 8 quarters for the quarterly.
#for the moment force the test to be as long as exactly one period
#we should implement a mechanism for doing repeated predictions
train <- tsObj$x
trainHier <- tsaggregates(train)
trainHier
library(hts)
library(huge)
source("loadTourism.R")
if (is.character(dset) == FALSE) {
stop ("dset should be a string")
}
bayesRecon <- function (correlation){
S <- smatrix(train)
bottomIdx <- seq( nrow(S) - ncol(S) +1, nrow(S))
upperIdx <- setdiff(1:nrow(S),bottomIdx)
#prior mean and covariance of the bottom time series
priorMean <- preds[bottomIdx]
Y_vec <- preds[upperIdx]
Sigma_y <- matrix(nrow = length(upperIdx), ncol = length(upperIdx))
#prior covariance for the bottom time series
bottomVar <- sigma[bottomIdx]^2
priorCov <- diag(bottomVar)
if (correlation){
#the covariances are the covariances of the time series
#the variances are the variances of the forecasts, hence the variances of the residuals
bottomResiduals <- residuals[,bottomIdx]
priorCov <- cov(bottomResiduals)
out.glasso <- huge(bottomResiduals, method = "glasso", cov.output = TRUE)
out.select <- huge.select(out.glasso, criterion = "stars")
priorCov <- out.select$opt.cov
}
#covariance for the upper time series
upperVar <- sigma[upperIdx]^2
Sigma_y <- diag(upperVar)
if (correlation){
#get variance and covariance of the residuals
upperResiduals <- residuals[,upperIdx]
Sigma_y <- cov(upperResiduals)
out.glasso <- huge(upperResiduals, method = "glasso", cov.output = TRUE)
out.select <- huge.select(out.glasso, criterion = "stars")
Sigma_y <- out.select$opt.cov
}
#==updating
#A explains how to combin the bottom series in order to obtain the
# upper series
A <- t(S[upperIdx,])
M <- ncol ( t(A) %*% priorCov %*% A + Sigma_y )
correl <- priorCov %*% A %*%
solve (t(A) %*% priorCov %*% A + Sigma_y + 1e-6*diag(M))
postMean <- priorMean + correl  %*%
(Y_vec - t(A) %*% priorMean)
bayesPreds <- buReconcile(postMean, S, predsAllTs = FALSE)
return(bayesPreds)
}
#The buReconcile function computes the bu prediction given the predictions (1 x tot time series) and the S matrix
#(tot time series X bottom time series)
#predsAllTs is a flag: is set to true, the input preds contains predictions for all the hierarchy
#and the function retrieves the bottom series; if set to false, this is not needed
#as preds only contains only bottom time series
buReconcile <- function (preds,S, predsAllTs = FALSE) {
bottomPreds <- preds
if (predsAllTs) {
#retrieves the bottom prediction from all predictions
upperIdx <- 1 : (nrow(S) - ncol(S))
bottomIdx <- setdiff (1:nrow(S), upperIdx)
bottomPreds <- preds [,bottomIdx]
}
buPreds <- preds
#nrow(S) is the total number of time series
for (i in 1:nrow(S)){
buPreds[i] <- S[i,] %*% bottomPreds
}
return (buPreds)
}
#check the calibration of the prediction interval with coverage (1-currentAlpha)
checkCalibration <- function(preds,sigmas,htsActual,coverage){
stdQuant <- abs(qnorm((1-coverage)/2))
included <- vector(length = length(preds))
actual <- allts(htsActual)[h,]
for (ii in seq_along(preds)){
upper <- preds[ii] + stdQuant * sigmas[ii]
lower <- preds[ii] - stdQuant * sigmas[ii]
included[ii] <- (actual[ii] > lower) &  (actual[ii] < upper)
}
return (mean(included))
}
hierMse <- function (htsPred, htsActual, h) {
#receives two hts objects, containing  forecast and actual value.
#computes the mse for the whole hierarchy.
mse <- mean  ( (allts(htsPred)[h,] - allts(htsActual)[h,])^2 )
return (mse)
}
#extract the time from the data set to then split into train / test (test set contains 25 or 5 time points)
set.seed(seed = 0)
hierTs <- loadTourism()
summary(hierT)
summary(hierTs)
library("thief")
?AEdemand
library(hts)
library(huge)
source("loadTourism.R")
bayesRecon <- function (correlation){
S <- smatrix(train)
bottomIdx <- seq( nrow(S) - ncol(S) +1, nrow(S))
upperIdx <- setdiff(1:nrow(S),bottomIdx)
#prior mean and covariance of the bottom time series
priorMean <- preds[bottomIdx]
Y_vec <- preds[upperIdx]
Sigma_y <- matrix(nrow = length(upperIdx), ncol = length(upperIdx))
#prior covariance for the bottom time series
bottomVar <- sigma[bottomIdx]^2
priorCov <- diag(bottomVar)
if (correlation){
#the covariances are the covariances of the time series
#the variances are the variances of the forecasts, hence the variances of the residuals
bottomResiduals <- residuals[,bottomIdx]
priorCov <- cov(bottomResiduals)
out.glasso <- huge(bottomResiduals, method = "glasso", cov.output = TRUE)
out.select <- huge.select(out.glasso, criterion = "stars")
priorCov <- out.select$opt.cov
}
#covariance for the upper time series
upperVar <- sigma[upperIdx]^2
Sigma_y <- diag(upperVar)
if (correlation){
#get variance and covariance of the residuals
upperResiduals <- residuals[,upperIdx]
Sigma_y <- cov(upperResiduals)
out.glasso <- huge(upperResiduals, method = "glasso", cov.output = TRUE)
out.select <- huge.select(out.glasso, criterion = "stars")
Sigma_y <- out.select$opt.cov
}
#==updating
#A explains how to combin the bottom series in order to obtain the
# upper series
A <- t(S[upperIdx,])
M <- ncol ( t(A) %*% priorCov %*% A + Sigma_y )
correl <- priorCov %*% A %*%
solve (t(A) %*% priorCov %*% A + Sigma_y + 1e-6*diag(M))
postMean <- priorMean + correl  %*%
(Y_vec - t(A) %*% priorMean)
bayesPreds <- buReconcile(postMean, S, predsAllTs = FALSE)
return(bayesPreds)
}
#The buReconcile function computes the bu prediction given the predictions (1 x tot time series) and the S matrix
#(tot time series X bottom time series)
#predsAllTs is a flag: is set to true, the input preds contains predictions for all the hierarchy
#and the function retrieves the bottom series; if set to false, this is not needed
#as preds only contains only bottom time series
buReconcile <- function (preds,S, predsAllTs = FALSE) {
bottomPreds <- preds
if (predsAllTs) {
#retrieves the bottom prediction from all predictions
upperIdx <- 1 : (nrow(S) - ncol(S))
bottomIdx <- setdiff (1:nrow(S), upperIdx)
bottomPreds <- preds [,bottomIdx]
}
buPreds <- preds
#nrow(S) is the total number of time series
for (i in 1:nrow(S)){
buPreds[i] <- S[i,] %*% bottomPreds
}
return (buPreds)
}
#check the calibration of the prediction interval with coverage (1-currentAlpha)
checkCalibration <- function(preds,sigmas,htsActual,coverage){
stdQuant <- abs(qnorm((1-coverage)/2))
included <- vector(length = length(preds))
actual <- allts(htsActual)[h,]
for (ii in seq_along(preds)){
upper <- preds[ii] + stdQuant * sigmas[ii]
lower <- preds[ii] - stdQuant * sigmas[ii]
included[ii] <- (actual[ii] > lower) &  (actual[ii] < upper)
}
return (mean(included))
}
hierMse <- function (htsPred, htsActual, h) {
#receives two hts objects, containing  forecast and actual value.
#computes the mse for the whole hierarchy.
mse <- mean  ( (allts(htsPred)[h,] - allts(htsActual)[h,])^2 )
return (mse)
}
#extract the time from the data set to then split into train / test (test set contains 25 or 5 time points)
set.seed(seed = 0)
loadTourism()
hierTs <- loadTourism()
suppressWarnings(hierTs <- loadTourism())
a <- rnorm(1000)
a <- matrix(data=rnorm(1000), nrow = 100, ncol = 10)
a <- as.data.frame(matrix(data=rnorm(1000), nrow = 100, ncol = 10))
a
library(purrr)
map_dbl(a,mean)
map_dbl(a,sd)
seq(0,2)
#linear kernel
linkern <- function(x,x1,var,c)
{
return (var * (x - c) * (x1 - c))
}
x <- seq (0,2)
y <- seq (0,2)
c <- mean(x)
var=1
kernelMat <- matrix(nrow=length(x), ncol = length(y))
for (i in seq_along(x)){
for (j in seq_along(y)){
kernelMat[i,j] <- linkern(x,y, var=var, c=c)
}
kernelMat <- matrix(nrow=length(x), ncol = length(y))
for (i in seq_along(x)){
for (j in seq_along(y)){
kernelMat[i,j] <- linkern(x[i],y[j], var=var, c=c)
}
kernelMat
x <- seq (0,4)
y <- seq (0,4)
c <- mean(x)
var=1
kernelMat <- matrix(nrow=length(x), ncol = length(y))
for (i in seq_along(x)){
for (j in seq_along(y)){
kernelMat[i,j] <- linkern(x[i],y[j], var=var, c=c)
}
kernelMat
install.packages("plotrix")
library(plotrix)
color2D.matplot(kernelMat)
library(mass)
library(MASS)
mu <- rep (0, length(x))
mu
mvrnorm(n=10, mu = )
?mvrnorm
mvrnorm(n=10, mu = mu, Sigma = kernelMat)
a <- mvrnorm(n=10, mu = mu, Sigma = kernelMat)
plot(a)
lines(a)
a <- mvrnorm(n=10, mu = mu, Sigma = kernelMat)
class(a)
matplot(a)
?matplot
matplot(a, type = "l")
#linear kernel
linkern <- function(x,x1,var,c)
{
return (var * (x - c) * (x1 - c))
}
x <- seq (0,4)
y <- seq (0,4)
c <- mean(x)
var=1
kernelMat <- matrix(nrow=length(x), ncol = length(y))
for (i in seq_along(x)){
for (j in seq_along(y)){
kernelMat[i,j] <- linkern(x[i],y[j], var=var, c=c)
}
library(plotrix)
color2D.matplot(kernelMat)
color2D.matplot(kernelMat, title("aa"))
color2D.matplot(kernelMat, title("Covariance of linear kernel"))
kernelMat <- matrix(nrow=length(x), ncol = length(y))
for (i in seq_along(x)){
for (j in seq_along(y)){
kernelMat[i,j] <- linkern(x[i],y[j], var=var, c=c)
}
library(plotrix)
color2D.matplot(kernelMat, title("Covariance of linear kernel"))
color2D.matplot(kernelMat, title="Covariance of linear kernel")
?color2D.matplot
library(MASS)
mu <- rep (0, length(x))
a <- mvrnorm(n=10, mu = mu, Sigma = kernelMat)
matplot(a, type = "l")
x
c
kernelMat
ourMu <- mu[,1]
mu <- rep (0, length(x))
a <- mvrnorm(n=10, mu = mu, Sigma = kernelMat)
ourMu <- mu[,1]
mu
mu <- rep (0, length(x))
a <- mvrnorm(n=10, mu = mu, Sigma = kernelMat)
a
ourMu <- a[,1]
ourMu
plot(ourMu)
mu <- rep (0, length(x))
sampledMu <- mvrnorm(n=1, mu = mu, Sigma = kernelMat)
samples <- mvrnorm(n=30, mu = sampledMu, Sigma = kernelMat)
matplot(samples, type = "l")
sampledMu <- mvrnorm(n=1, mu = mu, Sigma = kernelMat)
sampledMu
plot(sampledMu)
sampledMu
sampledMu <- mvrnorm(n=30, mu = mu, Sigma = kernelMat)
dim(sampledMu)
sampleds <- mvrnorm(n=30, mu = mu, Sigma = kernelMat)
matplot(t(samples), type = "l")
kernelMat
#linear kernel
linKern <- function(x,x1,var,c)
{
return (var * (x - c) * (x1 - c))
}
x <- seq (0,4)
y <- seq (0,4)
c <- mean(x)
var=1
kernelMat <- matrix(nrow=length(x), ncol = length(y))
for (i in seq_along(x)){
for (j in seq_along(y)){
kernelMat[i,j] <- linKern(x[i],y[j], var=var, c=c)
}
library(plotrix)
color2D.matplot(kernelMat)
library(plotrix)
color2D.matplot(kernelMat)
library(MASS)
mu <- rep (0, length(x))
sampleds <- mvrnorm(n=30, mu = mu, Sigma = kernelMat)
matplot(t(samples), type = "l")
library(MASS)
mu <- rep (0, length(x))
samples <- mvrnorm(n=30, mu = mu, Sigma = kernelMat)
matplot(t(samples), type = "l")
kernelMat
sigma4 <- sqrt(kernelMat[4,4])
sigma5 <- sqrt(kernelMat[5,5])
sigma4
sigma5
kernelMat[4,5]
kernelMat[4,5] * kernelMat[4,5]
(kernelMat[4,5] * kernelMat[4,5] ) / sigma4
sigma4
kernelMat
sigmaxx - sigmaxy * sigmaxy / kernelMat[1,1]
sigma5Given4 <- sigma5 - kernelMat[4,5] *
sigmaxx <-  kernelMat[2,2]
sigma5Given4 <- sigma5 - kernelMat[4,5] *
sigmaxx <-  kernelMat[2,2]
sigma5Given4 <- sigma5 - kernelMat[4,5] *
sigmaxx <-  kernelMat[2,2]
sigma5Given4 <- sigma5 - kernelMat[4,5]
sigmaxx <-  kernelMat[2,2]
sigmaxy <-  kernelMat[1,2]
sigmaxx - sigmaxy * sigmaxy / kernelMat[1,1]
#creating autocorrelogramm
library(fpp2)
ts1 <- window(ausbeer, start=c(1965,1), end=c(1967,4))
par(pty="s")
plot(as.numeric(ts1), ylab = "time series (y)", xlab="time (quarters)", type="o", axes=FALSE, ann=FALSE)
timestr<-c("Q1","Q2","Q3","Q4","Q1","Q2","Q3","Q4","Q1","Q2","Q3","Q4","Q1","Q2","Q3","Q4")
par(cex=1.2)
axis(1, at=1:length(ts1), lab=timestr[1:length(ts1)])
axis(2)
#Add vertical grid
abline(v = c(4,8,12),  lty = 2, col = "grey")
autoplot(ts1)
pdfname<-"ts1.pdf"
ggsave(pdfname, width = 4, height = 3)
pdfname<-"acf.pdf"
ggAcf(ts1)
#creating autocorrelogramm
library(fpp2)
ts1 <- window(ausbeer, start=c(1965,1), end=c(1967,4))
par(pty="s")
plot(as.numeric(ts1), ylab = "time series (y)", xlab="time (quarters)", type="o", axes=FALSE, ann=FALSE)
timestr<-c("Q1","Q2","Q3","Q4","Q1","Q2","Q3","Q4","Q1","Q2","Q3","Q4","Q1","Q2","Q3","Q4")
par(cex=1.2)
axis(1, at=1:length(ts1), lab=timestr[1:length(ts1)])
axis(2)
#Add vertical grid
abline(v = c(4,8,12),  lty = 2, col = "grey")
dev.off()
autoplot(ts1)
pdfname<-"ts1.pdf"
ggsave(pdfname, width = 4, height = 3)
pdfname<-"acf.pdf"
ggAcf(ts1)
a <- ggAcf(ts1)
a
a$data
